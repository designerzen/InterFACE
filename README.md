# InterFACE
A camera based musical instrument.

Smile to begin - click your face to change to a different instrument.


# Describe your entry (up to 1000 characters) *
A face controlled musical instrument that uses a video camera.

Or, an augmented reality, real-time motion-capture, machine-learning powered, holographic, acessible general MIDI controller and synthesizer (that comes without a manual). 

By piggybacking onto a person's own natural movements, we reduce the complexity required to learn a new instrument and this allow for expressive yet intuitive control.

I've made an interactive protoype that hopefully explains it better than these words have!

# Describe how it is innovative (up to 500 characters) *

Real-time motion tracking become possible due to innovations in machine learning and improvements in graphics processers.

This software has been designed to be used with the Looking Glass 'Portrait' display - the world's first consumer grade holographic display which gives the visuals extra depth and believability.

By creating a virtual interface with no moving parts or extra hardware, we can use a person's own expressions and movements to control and manipulate hardware, software and data.



# What is the most inspiring use of your entry (up to 500 characters)? *
I have been researching and creating accessible technology for twenty years and for the past 10 have been helping run a monthly workshop alongside Drake Music Charity that brings together people who struggle to play traditional instruments, and technology focussed individuals who know how to build and make things. The charity work directly alongside schools and individuals offering real world use cases and genuine feedback - indispensable when simplicity is the aim. Together we have created a whole number of accessible musical instruments, some that are repurposing of existing instruments and some entirely new. My focus for the past few years has been to try and create musical instruments & music creation tools with the lowest barriers to entry - intuitive tools that everybody can play instinctively, that are easy to obtain and fun to play too! Currently the only requirement for this one to play is a mouth, but that could be swapped for eyes or eyebrows, the technology is quite broad!

Ultimately, this tool allows *all* sorts of people to create their own sounds and music but the technology can ultimately be expanded to control anything.



# Do you have plans for expansion and commercialisation (up to 500 characters)?

Plans for expansion would be to add more musicality, expression and to create an app version that would allow it to run smoother on mobile devices. I think this would be key to getting it onto everybody's devices and the app stores also offers retail opportunities. 

Plans for commercialisation would revolve around monetisation from expansion packs or potentially a paid for pro version. For development to continue there would have to be a certain level of income provided by the project but I have open sourced it.

Already there are plans for a recording looper and effects but ideally the full magnitude of the project would be suggested by user cases in an attempt to streamline the simplicity and minimize development costs whilst still offering a viable product.


# Give examples of how your entry connects with other MIDI devices or software

Can be used as a controller for other hardware MIDI devices and in duet-mode allows 2 people to independently control individual MIDI channels in realtime together. 

In single player mode, the MIDI events currently get broadcast to all channels.

The prototype has only basic note on / note off / pitch bend MIDI interaction but I would like to add after touch to the interface, perhaps eye brow control? Eye direction currently controls pitch bend on MIDI but stereo panning on the synth.

I would also like to develop certain different modes - as well as the expressive mode which works in direct relation with facial movements (as it currently works) would be a more fun but equally satisfying pop music factory mode where the mouth controls the playback and amplitude of the next phrase in a sequence (loaded from a MIDI file) whilst a backing track supports you. I think that would be a lot of fun and smooths over the latency issues. 

Given enough time it would be possible to save MIDI files directly from the app too, turning it into a rudimentary face controlled DAW.


# Is your entry a commercialised product? * Please explain

Not currently (I have open sourced it on github) but it has applications that certainly could be and I would love to be able to fund further development, as although a one off holographic prototype will be made for use inside the forthcoming Accessible Musical Instrument Collection museum in London, there is no reason that it could not be commercialised and expanded into a more professional commercial product. The technology is very interesting and my prototype doesn't do justice to the real possibilities (neither visually or sonically) but I hope it illustrates the concept I have in mind and the potential of this approach!
